{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activity 01\n",
    "import langdetect\n",
    "import matplotlib.pyplot\n",
    "import nltk\n",
    "import numpy\n",
    "import pandas\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import regex\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'latimeshealth.txt'\n",
    "df = pandas.read_csv(path, sep=\"|\", header=None)\n",
    "df.columns = [\"id\", \"datetime\", \"tweettext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE:\n",
      "(4171, 3)\n",
      "\n",
      "COLUMN NAMES:\n",
      "Index(['id', 'datetime', 'tweettext'], dtype='object')\n",
      "\n",
      "HEAD:\n",
      "                   id                        datetime  \\\n",
      "0  576760256031682561  Sat Mar 14 15:02:15 +0000 2015   \n",
      "1  576715414811471872  Sat Mar 14 12:04:04 +0000 2015   \n",
      "\n",
      "                                           tweettext  \n",
      "0  Five new running shoes that aim to go the extr...  \n",
      "1  Gym Rat: Disq class at Crunch is intense worko...  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dataframe_quick_look(df, nrows):\n",
    "    print(\"SHAPE:\\n{shape}\\n\".format(shape=df.shape))\n",
    "    print(\"COLUMN NAMES:\\n{names}\\n\".format(names=df.columns))\n",
    "    print(\"HEAD:\\n{head}\\n\".format(head=df.head(nrows)))\n",
    "dataframe_quick_look(df, nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLINES:\n",
      "['Five new running shoes that aim to go the extra mile http://lat.ms/1ELp3wU', 'Gym Rat: Disq class at Crunch is intense workout on pulley system http://lat.ms/1EKOFdr', 'Noshing through thousands of ideas at Natural Products Expo West http://lat.ms/1EHqywg', 'Natural Products Expo also explores beauty, supplements and more http://lat.ms/1EHqyfE', 'Free Fitness Weekends in South Bay beach cities aim to spark activity http://lat.ms/1EH3SMC']\n",
      "\n",
      "LENGTH:\n",
      "4171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = df['tweettext'].tolist()\n",
    "print(\"HEADLINES:\\n{lines}\\n\".format(lines=raw[:5]))\n",
    "print(\"LENGTH:\\n{length}\\n\".format(length=len(raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_language_identifying(txt):\n",
    "    try:\n",
    "        the_language = langdetect.detect(txt)\n",
    "    except:\n",
    "        the_language = 'none'\n",
    "    return the_language\n",
    "\n",
    "\n",
    "def do_lemmatizing(wrd):\n",
    "    out = nltk.corpus.wordnet.morphy(wrd)\n",
    "    return (wrd if out is None else out)\n",
    "\n",
    "\n",
    "def do_tweet_cleaning(txt):\n",
    "# identify language of tweet\n",
    "# return null if language not english\n",
    "    lg = do_language_identifying(txt)\n",
    "    if lg != 'en':\n",
    "        return None\n",
    "\n",
    "# split the string on whitespace\n",
    "    out = txt.split(' ')\n",
    "\n",
    "# identify screen names\n",
    "# replace with SCREENNAME\n",
    "    out = ['SCREENNAME' if i.startswith('@') else i for i in out]\n",
    "\n",
    "# identify urls\n",
    "# replace with URL\n",
    "    out = ['URL' if bool(regex.search('http[s]?://', i)) else i for i in out]\n",
    "\n",
    "# remove all punctuation\n",
    "    out = [regex.sub('[^\\\\w\\\\s]|\\n', '', i) for i in out]\n",
    "\n",
    "# make all non-keywords lowercase\n",
    "    keys = ['SCREENNAME', 'URL']\n",
    "    out = [i.lower() if i not in keys else i for i in out]\n",
    "\n",
    "# remove keywords\n",
    "    out = [i for i in out if i not in keys]\n",
    "\n",
    "# remove stopwords\n",
    "    list_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    list_stop_words = [    regex.sub('[^\\\\w\\\\s]', '', i) for i in list_stop_words]\n",
    "\n",
    "    out = [i for i in out if i not in list_stop_words]\n",
    "\n",
    "# lemmatizing\n",
    "    out = [do_lemmatizing(i) for i in out]\n",
    "\n",
    "# keep words 4 or more characters long\n",
    "    out = [i for i in out if len(i) >= 5]\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = list(map(do_tweet_cleaning, raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLINES:\n",
      "[['running', 'shoes', 'extra'], ['class', 'crunch', 'intense', 'workout', 'pulley', 'system'], ['thousand', 'natural', 'product'], ['natural', 'product', 'explore', 'beauty', 'supplement'], ['fitness', 'weekend', 'south', 'beach', 'spark', 'activity']]\n",
      "\n",
      "LENGTH:\n",
      "4093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean = list(filter(None.__ne__, clean))\n",
    "print(\"HEADLINES:\\n{lines}\\n\".format(lines=clean[:5]))\n",
    "print(\"LENGTH:\\n{length}\\n\".format(length=len(clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running shoes extra', 'class crunch intense workout pulley system', 'thousand natural product', 'natural product explore beauty supplement', 'fitness weekend south beach spark activity', 'kayla harrison sacrifice', 'sonic treatment alzheimers disease', 'ultrasound brain restore memory alzheimers needle onlyso farin mouse', 'apple researchkit really medical research', 'warning chantix drink taking might remember']\n"
     ]
    }
   ],
   "source": [
    "clean_sentences = [\" \".join(i) for i in clean]\n",
    "print(clean_sentences[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activity 02\n",
    "number_words = 10\n",
    "number_docs = 10\n",
    "number_features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 320)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer1 = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    max_df=0.95, \n",
    "    min_df=10, \n",
    "    max_features=number_features\n",
    ")\n",
    "clean_vec1 = vectorizer1.fit_transform(clean_sentences)\n",
    "print(clean_vec1[0])\n",
    "\n",
    "feature_names_vec1 = vectorizer1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number Of Topics  Perplexity Score\n",
      "0                 2        349.004885\n",
      "1                 4        404.137619\n",
      "2                 6        440.677441\n",
      "3                 8        464.222793\n",
      "4                10        478.094739\n",
      "5                12        493.116250\n",
      "6                14        506.144776\n",
      "7                16        524.674504\n",
      "8                18        530.975575\n",
      "9                20        535.461393\n"
     ]
    }
   ],
   "source": [
    "def perplexity_by_ntopic(data, ntopics):\n",
    "    output_dict = {\n",
    "        \"Number Of Topics\": [], \n",
    "        \"Perplexity Score\": []\n",
    "    }\n",
    "    for t in ntopics:\n",
    "        lda = sklearn.decomposition.LatentDirichletAllocation(\n",
    "            n_components=t,\n",
    "            learning_method=\"online\",\n",
    "            random_state=0\n",
    "        )\n",
    "        lda.fit(data)\n",
    "        output_dict[\"Number Of Topics\"].append(t)\n",
    "        output_dict[\"Perplexity Score\"].append(lda.perplexity(data))\n",
    "    output_df = pandas.DataFrame(output_dict)\n",
    "    index_min_perplexity = output_df[\"Perplexity Score\"].idxmin()\n",
    "    output_num_topics = output_df.loc[\n",
    "        index_min_perplexity,  # index\n",
    "        \"Number Of Topics\"  # column\n",
    "    ]\n",
    "    return (output_df, output_num_topics)\n",
    "df_perplexity, optimal_num_topics = perplexity_by_ntopic(\n",
    "    clean_vec1, \n",
    "    ntopics=[i for i in range(1, 21) if i % 2 == 0]\n",
    ")\n",
    "print(df_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=2, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = sklearn.decomposition.LatentDirichletAllocation(\n",
    "    n_components=optimal_num_topics,\n",
    "    learning_method=\"online\",\n",
    "    random_state=0\n",
    ")\n",
    "lda.fit(clean_vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Topic0              Topic1\n",
      "Word0      (0.0417, latfit)     (0.0817, study)\n",
      "Word1      (0.0336, health)    (0.0306, cancer)\n",
      "Word2      (0.0242, people)   (0.0212, patient)\n",
      "Word3       (0.0203, could)     (0.0172, death)\n",
      "Word4       (0.0192, brain)    (0.017, obesity)\n",
      "Word5   (0.018, researcher)    (0.0168, doctor)\n",
      "Word6       (0.0176, woman)     (0.0166, heart)\n",
      "Word7       (0.016, report)   (0.0148, disease)\n",
      "Word8  (0.0143, california)    (0.0144, weight)\n",
      "Word9   (0.0125, scientist)  (0.0115, research)\n"
     ]
    }
   ],
   "source": [
    "def get_topics(mod, vec, names, docs, ndocs, nwords):\n",
    "    # word to topic matrix\n",
    "    W = mod.components_\n",
    "    W_norm = W / W.sum(axis=1)[:, numpy.newaxis]\n",
    "    # topic to document matrix\n",
    "    H = mod.transform(vec)\n",
    "    W_dict = {}\n",
    "    H_dict = {}\n",
    "    for tpc_idx, tpc_val in enumerate(W_norm):\n",
    "        topic = \"Topic{}\".format(tpc_idx)\n",
    "        # formatting w\n",
    "        W_indices = tpc_val.argsort()[::-1][:nwords]\n",
    "        W_names_values = [\n",
    "            (round(tpc_val[j], 4), names[j]) \n",
    "            for j in W_indices\n",
    "        ]\n",
    "        W_dict[topic] = W_names_values\n",
    "        # formatting h\n",
    "        H_indices = H[:, tpc_idx].argsort()[::-1][:ndocs]\n",
    "        H_names_values = [\n",
    "        (round(H[:, tpc_idx][j], 4), docs[j]) \n",
    "            for j in H_indices\n",
    "        ]\n",
    "        H_dict[topic] = H_names_values\n",
    "    W_df = pandas.DataFrame(\n",
    "        W_dict, \n",
    "        index=[\"Word\" + str(i) for i in range(nwords)]\n",
    "    )\n",
    "    H_df = pandas.DataFrame(\n",
    "        H_dict,\n",
    "        index=[\"Doc\" + str(i) for i in range(ndocs)]\n",
    "    )\n",
    "    return (W_df, H_df)\n",
    "\n",
    "W_df, H_df = get_topics(\n",
    "    mod=lda,\n",
    "    vec=clean_vec1,\n",
    "    names=feature_names_vec1,\n",
    "    docs=raw,\n",
    "    ndocs=number_docs, \n",
    "    nwords=number_words\n",
    ")\n",
    "\n",
    "print(W_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Topic0  \\\n",
      "Doc0  (0.9443, Want your legs to look good in those ...   \n",
      "Doc1  (0.9442, 11% of hospital patients got care the...   \n",
      "Doc2  (0.9373, Spend time with dad this Fatherâ€™s Day...   \n",
      "Doc3  (0.9373, Hve fun! That's an order. It's import...   \n",
      "Doc4  (0.9372, Need a new challenge for your ab work...   \n",
      "Doc5  (0.9368, ZMapp goes 18-for-18 in treating monk...   \n",
      "Doc6  (0.9367, Anti-vaccination activists target hig...   \n",
      "Doc7  (0.9337, RT @latimesscience: @xprize pulled th...   \n",
      "Doc8  (0.9285, About 75% of homeless people smoke, a...   \n",
      "Doc9  (0.9284, Yogi crunches can give you flat abs a...   \n",
      "\n",
      "                                                 Topic1  \n",
      "Doc0  (0.9498, Computer problems are delaying nursin...  \n",
      "Doc1  (0.9457, Trans fats? DONE. Will the @US_FDA go...  \n",
      "Doc2  (0.9414, Supplements to boost \"low T\" increase...  \n",
      "Doc3  (0.9372, Study: The 2009 H1N1 \"swine flu\" pand...  \n",
      "Doc4  (0.9363, Doctors often delay vaccines for youn...  \n",
      "Doc5  (0.9357, Humans eat more calories, protein and...  \n",
      "Doc6  (0.9356, Las Vegas: Finding the latest in bike...  \n",
      "Doc7  (0.9354, Soccer players' ACL injury risk may d...  \n",
      "Doc8  (0.9284, Men walk more slowly with a woman IF ...  \n",
      "Doc9  (0.9284, Do blood transfusions from Ebola surv...  \n"
     ]
    }
   ],
   "source": [
    "print(H_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rutujay\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el346023839980536162202706808\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el346023839980536162202706808_data = {\"mdsDat\": {\"x\": [0.2473270705432186, -0.2473270705432186], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [50.23018464334628, 49.76981535665372]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"Freq\": [489.0, 244.0, 196.0, 180.0, 142.0, 126.0, 119.0, 113.0, 106.0, 103.0, 180.26118170468703, 97.64979655324052, 87.40112615688545, 98.689118108569, 84.60139896073954, 61.479698098249855, 56.87539556647292, 60.61654426764612, 99.91952208283514, 49.660635894377585, 481.31340184616016, 124.97191402240725, 101.12665464372093, 67.5532447120477, 243.45690641152012, 196.04069505377439, 141.43302637789768, 105.27839636669333, 83.7004879900076, 112.11762940575048, 93.07929234775875, 118.50631076466682, 54.85311886203236, 45.571563192746645, 102.61836611706349, 69.13477645819655, 71.89659538057204, 73.12082681144344], \"Term\": [\"study\", \"latfit\", \"health\", \"cancer\", \"people\", \"patient\", \"could\", \"brain\", \"researcher\", \"woman\", \"cancer\", \"heart\", \"disease\", \"doctor\", \"weight\", \"treatment\", \"breast\", \"better\", \"obesity\", \"increase\", \"study\", \"patient\", \"death\", \"research\", \"latfit\", \"health\", \"people\", \"researcher\", \"california\", \"brain\", \"report\", \"could\", \"today\", \"court\", \"woman\", \"expert\", \"medical\", \"scientist\"], \"Total\": [489.0, 244.0, 196.0, 180.0, 142.0, 126.0, 119.0, 113.0, 106.0, 103.0, 180.85600733936545, 98.21325696596762, 87.97926058252737, 99.54015329250757, 85.41834835243708, 62.0884571820103, 57.44251358912282, 61.280197180609704, 101.04176071162384, 50.27102043743702, 489.70617413306195, 126.73171488458694, 102.54691531174794, 68.96022104210775, 244.17847229926096, 196.6255725877058, 142.15685404313405, 106.05312324456325, 84.33190802724641, 113.05881896415383, 93.94505805951238, 119.64933163037328, 55.39082157356352, 46.100608495999374, 103.92804929743983, 69.97152771168794, 73.90708001558336, 87.27839722038065], \"loglift\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.6853, 0.6828, 0.682, 0.68, 0.6789, 0.6787, 0.6786, 0.6777, 0.6774, 0.6763, 0.6713, 0.6746, 0.6746, 0.6679, 0.6948, 0.6948, 0.6927, 0.6904, 0.6902, 0.6894, 0.6885, 0.6882, 0.688, 0.6862, 0.6851, 0.6857, 0.6702, 0.5208], \"logprob\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.4864, -4.0995, -4.2103, -4.0889, -4.2429, -4.5621, -4.64, -4.5763, -4.0765, -4.7756, -2.5043, -3.8528, -4.0645, -4.4679, -3.1767, -3.3933, -3.7198, -4.015, -4.2444, -3.9521, -4.1382, -3.8967, -4.667, -4.8523, -4.0406, -4.4356, -4.3964, -4.3795]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [0.9954276064128207, 0.016318485351029848, 0.008844953530932051, 0.9906347954643898, 0.9922964097238494, 0.017408708942523672, 0.011857907918754957, 0.9960642651754165, 0.9952669123245699, 0.005529260624025388, 0.008357756674222387, 0.994573044232464, 0.021691687650647395, 0.9978176319297801, 0.984915047838882, 0.009751634137018633, 0.9888694156322353, 0.011366315122209602, 0.9945735135556776, 0.010046197106623005, 0.014291527321232997, 0.9861153851650768, 0.005085808457360983, 0.9968184576427527, 0.9978286336024726, 0.010181924832678293, 0.9946088136847289, 0.01989217627369458, 0.004095365126105045, 0.9951737256435258, 0.027061006869413574, 0.9741962472988887, 0.9896898004915309, 0.009896898004915309, 0.9863355839052286, 0.015781369342483656, 0.007034483189228247, 0.9918621296811828, 0.01064451947399425, 0.9899403110814653, 0.9860757255763228, 0.014501113611416512, 0.009429236682581759, 0.9900698516710846, 0.16040624536962528, 0.8364039937130461, 0.9822216369877821, 0.016336326602707396, 0.01805353254549436, 0.9929442900021896, 0.9824692506238394, 0.0161060532889154, 0.9951023596158641, 0.01170708658371605, 0.00962204146772756, 0.9910702711759386], \"Term\": [\"better\", \"better\", \"brain\", \"brain\", \"breast\", \"breast\", \"california\", \"california\", \"cancer\", \"cancer\", \"could\", \"could\", \"court\", \"court\", \"death\", \"death\", \"disease\", \"disease\", \"doctor\", \"doctor\", \"expert\", \"expert\", \"health\", \"health\", \"heart\", \"heart\", \"increase\", \"increase\", \"latfit\", \"latfit\", \"medical\", \"medical\", \"obesity\", \"obesity\", \"patient\", \"patient\", \"people\", \"people\", \"report\", \"report\", \"research\", \"research\", \"researcher\", \"researcher\", \"scientist\", \"scientist\", \"study\", \"study\", \"today\", \"today\", \"treatment\", \"treatment\", \"weight\", \"weight\", \"woman\", \"woman\"]}, \"R\": 10, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el346023839980536162202706808\", ldavis_el346023839980536162202706808_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el346023839980536162202706808\", ldavis_el346023839980536162202706808_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el346023839980536162202706808\", ldavis_el346023839980536162202706808_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_plot = pyLDAvis.sklearn.prepare(lda, clean_vec1, vectorizer1, R=10)\n",
    "pyLDAvis.display(lda_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ACTIVITY 03\n",
    "vectorizer2 = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    max_df=0.5, \n",
    "    min_df=20, \n",
    "    max_features=number_features,\n",
    "    smooth_idf=False\n",
    ")\n",
    "clean_vec2 = vectorizer2.fit_transform(clean_sentences)\n",
    "print(clean_vec2[0])\n",
    "\n",
    "feature_names_vec2 = vectorizer2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.1, beta_loss='frobenius', init='nndsvda', l1_ratio=0.5,\n",
       "  max_iter=200, n_components=2, random_state=0, shuffle=False, solver='mu',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf = sklearn.decomposition.NMF(\n",
    "    n_components=optimal_num_topics,\n",
    "    init=\"nndsvda\",\n",
    "    solver=\"mu\",\n",
    "    beta_loss=\"frobenius\",\n",
    "    random_state=0, \n",
    "    alpha=0.1, \n",
    "    l1_ratio=0.5\n",
    ")\n",
    "nmf.fit(clean_vec2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Topic0                Topic1\n",
      "Word0    (0.3794, study)      (0.5955, latfit)\n",
      "Word1   (0.0256, cancer)       (0.0487, steps)\n",
      "Word2   (0.0207, people)       (0.0446, today)\n",
      "Word3  (0.0183, obesity)    (0.0402, exercise)\n",
      "Word4    (0.0183, brain)  (0.0273, healthtips)\n",
      "Word5   (0.0182, health)     (0.0258, workout)\n",
      "Word6  (0.0175, suggest)     (0.0203, getting)\n",
      "Word7   (0.0167, weight)     (0.0192, fitness)\n",
      "Word8    (0.0152, woman)       (0.0143, great)\n",
      "Word9     (0.013, death)     (0.0131, morning)\n"
     ]
    }
   ],
   "source": [
    "W_df, H_df = get_topics(\n",
    "    mod=nmf,\n",
    "    vec=clean_vec2,\n",
    "    names=feature_names_vec2,\n",
    "    docs=raw,\n",
    "    ndocs=number_docs, \n",
    "    nwords=number_words\n",
    ")\n",
    "\n",
    "print(W_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
